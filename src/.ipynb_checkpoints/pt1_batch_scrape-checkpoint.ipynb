{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Celebrity Death Data (via Wikipedia)\n",
    "\n",
    "Creating a notebook to produce the dataset found at the [Kaggle Celebrity Deaths Page](https://www.kaggle.com/hugodarwood/celebrity-deaths).\n",
    "\n",
    "Attempting to replace the current dataset since it isn't complete (up-to-date) since there's no notebook to run to get up-to-date information and it has bad parses for some of the fields.\n",
    "\n",
    "**Current branch: Consolidating pipeline into one notebook while implementing batch queries.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import csv\n",
    "import requests\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import sys\n",
    "#reload(sys)\n",
    "#sys.setdefaultencoding('utf-8')\n",
    "\n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch query Wikipedia for monthly death pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date range: 2004 - 2016\n"
     ]
    }
   ],
   "source": [
    "# batch query limit is 50\n",
    "year_list = range(2004,2017)\n",
    "print 'Date range:', min(year_list), '-', max(year_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "month_to_num = {\n",
    "    'January': 1,\n",
    "    'February': 2,\n",
    "    'March': 3,\n",
    "    'April': 4,\n",
    "    'May': 5,\n",
    "    'June': 6,\n",
    "    'July': 7,\n",
    "    'August': 8,\n",
    "    'September': 9,\n",
    "    'October': 10,\n",
    "    'November': 11,\n",
    "    'December': 12\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# max number of terms in one Wikipedia batch query\n",
    "api_qmax = 50\n",
    "\n",
    "# \n",
    "mo_yr_url_prefix = 'https://en.wikipedia.org/w/api.php?action=query&titles='\n",
    "mo_yr_url_suffix = '&prop=revisions&rvprop=content&format=json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions from previous version (master branch) that ran individual queries (instead of batch queries) in serial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "desc_death_re = re.compile('(.*?),? ?((?:.+)*?). (?:.*?)?', re.DOTALL|re.MULTILINE)\n",
    "death_clean_no_url_re = re.compile('\\s?(.[^<]+)\\.? ?(?=<|\\[?http)(?:.*)?$')\n",
    "# death_clean_no_url_re = re.compile('\\s?\\w(.[^<])+[.]?(?:<|\\[?http)(?:.*)?$')\n",
    "\n",
    "\"\"\"\n",
    "Input: single text string to be processed\n",
    "\n",
    "Output: list of two string elements\n",
    "  - first string is description of person\n",
    "  - second string is cause of death \n",
    "  (last clause of input when more than one comma in field)\n",
    "  \n",
    "\"\"\"\n",
    "# bad design below: just remove urls and refs instead of extracting\n",
    "def get_description_and_death(text):\n",
    "    text_no_url = text\n",
    "    has_death_urls = death_clean_no_url_re.match(text)\n",
    "    \n",
    "    if has_death_urls:\n",
    "        text_no_url = has_death_urls.groups()[0]\n",
    "    else:\n",
    "        text_no_url = text\n",
    "        \n",
    "    text_parts = text_no_url.replace('=','').split(',')\n",
    "    num_parts = len(text_parts)\n",
    "    \n",
    "    if num_parts == 0:\n",
    "        return ['', '']\n",
    "    elif num_parts == 1:\n",
    "        return text_parts + ['']\n",
    "    else:\n",
    "        return ([\",\".join(text_parts[:-1])] + [text_parts[-1]])\n",
    "    return text\n",
    "    \n",
    "\"\"\"\n",
    "Runs get_description_and_death() on the last element of a list\n",
    "\n",
    "Input: list of length n\n",
    "Output: list of length (n+1) with last element broken into description and death\n",
    "\"\"\"\n",
    "def add_description_and_death(entry_list):\n",
    "    return entry_list[:-1] + get_description_and_death(entry_list[-1])\n",
    "\n",
    "mo_yr_key_re = re.compile('(\\d+)_(\\d+).*?')\n",
    "name_age_re = re.compile('\\s?\\[\\[(.*?)\\]\\], (\\d+), (.+)?$', re.MULTILINE)\n",
    "\n",
    "\"\"\"\n",
    "Add the month and year as elements to an entry of type list\n",
    "\"\"\"\n",
    "def add_month_year_list(entry_list, mo_yr_key='_'):\n",
    "    base_list = mo_yr_key.split('_')\n",
    "    base_list.extend(entry_list)\n",
    "    return base_list\n",
    "\n",
    "\"\"\"\n",
    "Inputs: month-year key string, text entry string\n",
    "Outputs: list of length 4 of month, year, name, and age\n",
    "\"\"\"\n",
    "def parse_month_year_name_age(text_entry):\n",
    "    out_text = text_entry.replace('\\n', '')\n",
    "    yr_age_match = re.match(name_age_re, out_text)\n",
    "    if yr_age_match:\n",
    "        out_text = yr_age_match.groups()\n",
    "        return list(out_text)\n",
    "    return\n",
    "\n",
    "link_re = re.compile('\\[\\[([^\\|\\]]*)(?=\\||\\]\\])', re.DOTALL)\n",
    "link_all_re = re.compile('(\\[\\[(?:[^\\[\\]])+\\]\\])')\n",
    "\n",
    "\"\"\"\n",
    "Used to be messy, not anymore!\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Find wikitext links and convert them to the displayed text\n",
    "\n",
    "Input: text block\n",
    "Output: text block with wikitext URL text extracted and URL characters removed\n",
    "\"\"\"\n",
    "def extract_link_text(link_block):\n",
    "    link_present = link_re.search(link_block)\n",
    "    if link_present:\n",
    "        return link_present.groups()\n",
    "    return link_block\n",
    "\n",
    "\"\"\"\n",
    "Helper function for removing link text when using re.sub--identifies a wikitext URL\n",
    "\n",
    "Input: re.match object\n",
    "Output: text of matched object \n",
    "\"\"\"\n",
    "def link_only(matchobj):\n",
    "    cleaned_text = extract_link_text(matchobj.groups()[0])[0]\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Substitute all wikitext URL links with the display text for the URL\n",
    "\n",
    "Input: text block\n",
    "Output: text block with links removed\n",
    "\"\"\"\n",
    "def remove_link_text(text_block):\n",
    "    return re.sub(link_all_re, link_only, text_block)\n",
    "\n",
    "\n",
    "natl_pattern1 = re.compile(' ?((?:[A-Z][^\\s]+ ?)+) ', re.UNICODE)\n",
    "\n",
    "natl_unmatched_list = []\n",
    "\n",
    "def get_nationality_text(desc_text):\n",
    "    natl_match = natl_pattern1.match(desc_text.strip('['))\n",
    "    if natl_match:\n",
    "        return natl_match.groups()[0]\n",
    "    natl_unmatched_list.append(desc_text)\n",
    "    return desc_text\n",
    "    \n",
    "        \n",
    "# essentially does the same thing as extract_link_text\n",
    "def get_wiki_url(name_text):\n",
    "    return name_text.split('|')[0].strip('[').strip(']')\n",
    "\n",
    "\n",
    "def remove_end_period(text):\n",
    "    return re.sub('\\.$', '', re.sub('\\s$','',text))\n",
    "\n",
    "def remove_beginning_space(text):\n",
    "    return re.sub('^ +','',text)\n",
    "\n",
    "def clean_text(text):\n",
    "    if type(text) != str:\n",
    "        return text\n",
    "    \n",
    "    new_text = text\n",
    "    url_match = re.match(death_clean_no_url_re, text)\n",
    "    if url_match:\n",
    "        new_text = url_match.groups()[0]\n",
    "    return remove_beginning_space(\n",
    "        remove_end_period(\n",
    "            remove_link_text(new_text)\n",
    "        ).replace('[','').replace(']','')\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New functions in this branch/notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Convert Wikipedia 'Deaths in (str: month) (int: year)' titles into '(int: month)_(int: year)'\n",
    "\"\"\"\n",
    "date_eol_re = re.compile('([A-Z][a-z]+) (\\d{4})$')\n",
    "\n",
    "def month_str2key(month_str):\n",
    "    date_eol = re.search(date_eol_re, month_str)\n",
    "    if date_eol:\n",
    "        date_parts = date_eol.groups()\n",
    "        month_num = str(month_to_num[date_parts[0]])\n",
    "        year_num = date_parts[1]\n",
    "        return year_num + '_' + month_num\n",
    "    return month_str\n",
    "\n",
    "\"\"\"\n",
    "Remove URLs from a text block\n",
    "\"\"\"\n",
    "no_url_re = re.compile('\\[?https?:\\/\\/.*[\\r\\n]*', flags=re.MULTILINE)\n",
    "\n",
    "def remove_urls(text):\n",
    "    return re.sub(no_url_re, '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert(month_str2key('asdfasdf asdf asdf December 2013') == '2013_12')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API Batch Scrape\n",
    "\n",
    "To avoid API call limits and throttling. Also makes queries faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List of monthly death pages to be queried"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create individual search terms for API query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "death_rep_words = 'Deaths_in_'\n",
    "mo_yr_elems = [death_rep_words + month + '_' + str(year) \n",
    "              for month in month_to_num.keys()\n",
    "              for year in year_list]\n",
    "\n",
    "num_mo_yr_elems = len(mo_yr_elems)\n",
    "num_mo_yr_batch_queries = num_mo_yr_elems / api_qmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group query terms into batches of 50 (Wikipedia API's batch query limit):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mo_yr_batch_queries = []\n",
    "\n",
    "# collect entries into groups of 50\n",
    "for i in xrange(0,num_mo_yr_batch_queries):\n",
    "    start_idx = i * api_qmax\n",
    "    end_idx = start_idx + api_qmax\n",
    "    mo_yr_batch_queries.append(\"|\".join(mo_yr_elems[start_idx:end_idx]))\n",
    "\n",
    "# remaining entries\n",
    "mo_yr_batch_queries.append(\"|\".join(mo_yr_elems[end_idx:num_mo_yr_elems]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query Wikipedia API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch queries to Wikipedia API stored as a list of results for each batch query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 836 ms, sys: 133 ms, total: 969 ms\n",
      "Wall time: 12.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# query API\n",
    "mo_yr_batch_results = []\n",
    "\n",
    "for mo_yr_batch_query in mo_yr_batch_queries:\n",
    "    batch_url = mo_yr_url_prefix + mo_yr_batch_query + mo_yr_url_suffix\n",
    "    json_ret_val = requests.get(batch_url).json()\n",
    "    mo_yr_batch_results.append(json_ret_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q_headers = []\n",
    "q_contents = []\n",
    "\n",
    "for result in mo_yr_batch_results:\n",
    "    new_headers = [month_str2key(date_elem.values()[0]) \n",
    "                   for date_elem in result['query']['normalized']]\n",
    "    q_headers.extend(new_headers)\n",
    "    \n",
    "    new_contents = [[page_result['title'], page_result['revisions'][0]['*']] \n",
    "                    for page_result in result['query']['pages'].values()]\n",
    "    q_contents.extend(new_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unpacking queries into lists of summaries by month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.68 s, sys: 121 ms, total: 1.8 s\n",
      "Wall time: 1.78 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "q_contents_dict = {}\n",
    "\n",
    "for q_page in q_contents:\n",
    "    q_key = month_str2key(q_page[0])\n",
    "    q_list = [\n",
    "        add_month_year_list(\n",
    "            add_description_and_death(\n",
    "                parse_month_year_name_age(remove_urls(entry))),\n",
    "        q_key)\n",
    "        for entry in q_page[1].encode('utf-8').rstrip().split('*')\n",
    "        if re.match(name_age_re, entry.replace('\\n', ''))\n",
    "    ]\n",
    "    q_contents_dict[q_key] = q_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_entries = [entry for entry_list in q_contents_dict.values() for entry in entry_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55492, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013</td>\n",
       "      <td>9</td>\n",
       "      <td>Zvonko Bušić</td>\n",
       "      <td>67</td>\n",
       "      <td>Croatian airplane hijacker ([[TWA Flight 355]])</td>\n",
       "      <td>suicide by gunshot.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013</td>\n",
       "      <td>9</td>\n",
       "      <td>Joaquim Justino Carreira</td>\n",
       "      <td>63</td>\n",
       "      <td>Portuguese-born Brazilian Roman Catholic prelate</td>\n",
       "      <td>Bishop of [[Roman Catholic Diocese of Guarulh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013</td>\n",
       "      <td>9</td>\n",
       "      <td>Pál Csernai</td>\n",
       "      <td>80</td>\n",
       "      <td>Hungarian footballer and manager ([[FC Bayern ...</td>\n",
       "      <td>[[North Korea national football team|North Ko...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013</td>\n",
       "      <td>9</td>\n",
       "      <td>Ignacio Eizaguirre</td>\n",
       "      <td>92</td>\n",
       "      <td>Spanish footballer ([[Valencia CF|Valencia]], ...</td>\n",
       "      <td>[[Spain national football team|national team]]).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013</td>\n",
       "      <td>9</td>\n",
       "      <td>Ole Ernst</td>\n",
       "      <td>73</td>\n",
       "      <td>Danish actor.</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0  1                         2   3  \\\n",
       "0  2013  9              Zvonko Bušić  67   \n",
       "1  2013  9  Joaquim Justino Carreira  63   \n",
       "2  2013  9               Pál Csernai  80   \n",
       "3  2013  9        Ignacio Eizaguirre  92   \n",
       "4  2013  9                 Ole Ernst  73   \n",
       "\n",
       "                                                   4  \\\n",
       "0    Croatian airplane hijacker ([[TWA Flight 355]])   \n",
       "1   Portuguese-born Brazilian Roman Catholic prelate   \n",
       "2  Hungarian footballer and manager ([[FC Bayern ...   \n",
       "3  Spanish footballer ([[Valencia CF|Valencia]], ...   \n",
       "4                                      Danish actor.   \n",
       "\n",
       "                                                   5  \n",
       "0                                suicide by gunshot.  \n",
       "1   Bishop of [[Roman Catholic Diocese of Guarulh...  \n",
       "2   [[North Korea national football team|North Ko...  \n",
       "3   [[Spain national football team|national team]]).  \n",
       "4                                                     "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full = pd.DataFrame(all_entries)\n",
    "print df_full.shape\n",
    "df_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Pooling Multi-Processing Scrape\n",
    "\n",
    "For additional speed. However, might be very taxing on Wikipedia servers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
