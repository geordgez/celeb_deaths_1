{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Celebrity Death Data (via Wikipedia)\n",
    "\n",
    "Creating a notebook to produce the dataset found at the [Kaggle Celebrity Deaths Page](https://www.kaggle.com/hugodarwood/celebrity-deaths).\n",
    "\n",
    "Attempting to replace the current dataset since it isn't complete (up-to-date) since there's no notebook to run to get up-to-date information and it has bad parses for some of the fields.\n",
    "\n",
    "# Part I: Scraping Raw Pages, Local Download\n",
    "\n",
    "This notebook gets all Wikipedia death summary pages and stores them locally as JSON files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description\n",
    "\n",
    "This notebook downloads all monthly death lists to a local folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import csv\n",
    "import requests\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import sys\n",
    "#reload(sys)\n",
    "#sys.setdefaultencoding('utf-8')\n",
    "\n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "month_to_num = {\n",
    "    'January': 1,\n",
    "    'February': 2,\n",
    "    'March': 3,\n",
    "    'April': 4,\n",
    "    'May': 5,\n",
    "    'June': 6,\n",
    "    'July': 7,\n",
    "    'August': 8,\n",
    "    'September': 9,\n",
    "    'October': 10,\n",
    "    'November': 11,\n",
    "    'December': 12\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# batch query limit is 50\n",
    "year_list = range(2004,2007)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape all monthly death pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrape all death pages and store into the '../out/raw_pages' directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_url_prefix = 'https://en.wikipedia.org/w/api.php?action=query&titles='\n",
    "base_url_suffix = '&prop=revisions&rvprop=content&format=json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "%%time\n",
    "for year in year_list:\n",
    "    for month in month_to_num.keys():\n",
    "        filename = str(year) + '_' + str(month_to_num[month]) + '_deaths.json'\n",
    "        url = (base_url_prefix +  \n",
    "               'Deaths_in_' +\n",
    "               month + '_' + str(year) + \n",
    "               base_url_suffix)\n",
    "        content = requests.get(url).json()\n",
    "        with open(\"../out/raw_pages/\" + filename, \"wb\") as outfile:\n",
    "            json.dump(content, outfile)\n",
    "            outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "desc_death_re = re.compile('(.*?),? ?((?:.+)*?). (?:.*?)?', re.DOTALL|re.MULTILINE)\n",
    "death_clean_no_url_re = re.compile('\\s?(.[^<]+)\\.? ?(?=<|\\[?http)(?:.*)?$')\n",
    "# death_clean_no_url_re = re.compile('\\s?\\w(.[^<])+[.]?(?:<|\\[?http)(?:.*)?$')\n",
    "\n",
    "\"\"\"\n",
    "Input: single text string to be processed\n",
    "\n",
    "Output: list of two string elements\n",
    "  - first string is description of person\n",
    "  - second string is cause of death \n",
    "  (last clause of input when more than one comma in field)\n",
    "  \n",
    "\"\"\"\n",
    "# bad design below: just remove urls and refs instead of extracting\n",
    "def get_description_and_death(text):\n",
    "    text_no_url = text\n",
    "    if ('http' in text_no_url) or ('<ref' in text_no_url):\n",
    "        # bad/sloppy design here - test if match instead of catching exceptions\n",
    "        try:\n",
    "            text_no_url = death_clean_no_url_re.match(text).groups()[0]\n",
    "        except AttributeError:\n",
    "            print text\n",
    "            raw_input(\"Press enter to continue\")\n",
    "            raw_input(\"Press enter to continue\")\n",
    "    text_parts = text_no_url.replace('=','').split(',')\n",
    "    num_parts = len(text_parts)\n",
    "    if num_parts == 0:\n",
    "        return ['', '']\n",
    "    elif num_parts == 1:\n",
    "        return text_parts + ['']\n",
    "    else:\n",
    "        return ([\",\".join(text_parts[:-1])] + [text_parts[-1]])\n",
    "    return\n",
    "    \n",
    "\"\"\"\n",
    "Runs get_description_and_death() on the last element of a list\n",
    "\n",
    "Input: list of length n\n",
    "Output: list of length (n+1) with last element broken into description and death\n",
    "\"\"\"\n",
    "def add_description_and_death(entry_list):\n",
    "    return entry_list[:-1] + get_description_and_death(entry_list[-1])\n",
    "\n",
    "mo_yr_key_re = re.compile('(\\d+)_(\\d+).*?')\n",
    "name_age_re = re.compile('\\s?\\[\\[(.*?)\\]\\], (\\d+), (.+)?$', re.MULTILINE)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Inputs: month-year key string, text entry string\n",
    "Outputs: list of length 4 of month, year, name, and age\n",
    "\"\"\"\n",
    "def parse_month_year_name_age(my_key, text_entry):\n",
    "    return (list(re.match(mo_yr_key_re, my_key).groups()) +\n",
    "            list(re.match(name_age_re, text_entry.replace('\\n', '')).groups()))\n",
    "\n",
    "link_re = re.compile('\\[\\[([^\\|\\]]*)(?=\\||\\]\\])', re.DOTALL)\n",
    "link_all_re = re.compile('(\\[\\[(?:[^\\[\\]])+\\]\\])')\n",
    "\n",
    "\"\"\"\n",
    "Used to be messy, not anymore!\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Find wikitext links and convert them to the displayed text\n",
    "\n",
    "Input: text block\n",
    "Output: text block with wikitext URL text extracted and URL characters removed\n",
    "\"\"\"\n",
    "def extract_link_text(link_block):\n",
    "    link_present = link_re.search(link_block)\n",
    "    if link_present:\n",
    "        return link_present.groups()\n",
    "    return link_block\n",
    "\n",
    "\"\"\"\n",
    "Helper function for removing link text when using re.sub--identifies a wikitext URL\n",
    "\n",
    "Input: re.match object\n",
    "Output: text of matched object \n",
    "\"\"\"\n",
    "def link_only(matchobj):\n",
    "    cleaned_text = extract_link_text(matchobj.groups()[0])[0]\n",
    "    return cleaned_text\n",
    "\n",
    "\"\"\"\n",
    "Testing function\n",
    "\"\"\"\n",
    "def link_only_special(text):\n",
    "    print text.groups()\n",
    "\n",
    "\"\"\"\n",
    "Substitute all wikitext URL links with the display text for the URL\n",
    "\n",
    "Input: text block\n",
    "Output: text block with links removed\n",
    "\"\"\"\n",
    "def remove_link_text(text_block):\n",
    "    return re.sub(link_all_re, link_only, text_block)\n",
    "\n",
    "\n",
    "natl_pattern1 = re.compile(' ?((?:[A-Z][^\\s]+ ?)+) ', re.UNICODE)\n",
    "\n",
    "natl_unmatched_list = []\n",
    "\n",
    "def get_nationality_text(desc_text):\n",
    "    natl_match = natl_pattern1.match(desc_text.strip('['))\n",
    "    if natl_match:\n",
    "        return natl_match.groups()[0]\n",
    "    else:\n",
    "        natl_unmatched_list.append(desc_text)\n",
    "    return\n",
    "    \n",
    "        \n",
    "# essentially does the same thing as extract_link_text\n",
    "def get_wiki_url(name_text):\n",
    "    return name_text.split('|')[0].strip('[').strip(']')\n",
    "\n",
    "\n",
    "def remove_end_period(text):\n",
    "    return re.sub('\\.$', '', re.sub('\\s$','',text))\n",
    "\n",
    "def remove_beginning_space(text):\n",
    "    return re.sub('^ +','',text)\n",
    "\n",
    "def text_clean(text):\n",
    "    if type(text) != str:\n",
    "        return text\n",
    "    \n",
    "    new_text = text\n",
    "    url_match = re.match(death_clean_no_url_re, text)\n",
    "    if url_match:\n",
    "        new_text = url_match.groups()[0]\n",
    "    return remove_beginning_space(\n",
    "    remove_end_period(\n",
    "        remove_link_text(new_text)\n",
    "        ).replace('[','').replace(']','')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API Batch Scrape\n",
    "\n",
    "To avoid API call limits and throttling. Also makes queries faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_year_titles = []\n",
    "death_rep_words = 'Deaths_in_'\n",
    "\n",
    "for year in year_list:\n",
    "    month_titles_list = []\n",
    "    \n",
    "    for month in month_to_num.keys():\n",
    "        month_titles_list.append(death_rep_words + month + '_' + str(year))\n",
    "        \n",
    "    year_base_str = '|'.join(month_titles_list)\n",
    "    batch_year_titles.append(year_base_str)\n",
    "\n",
    "batch_year_str = '|'.join(batch_year_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Deaths_in_February_2004|Deaths_in_October_2004|Deaths_in_January_2004|Deaths_in_April_2004|Deaths_in_November_2004|Deaths_in_March_2004|Deaths_in_August_2004|Deaths_in_May_2004|Deaths_in_December_2004|Deaths_in_June_2004|Deaths_in_September_2004|Deaths_in_July_2004'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_year_titles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_url1 = base_url_prefix + batch_year_titles[0] + base_url_suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://en.wikipedia.org/w/api.php?action=query&titles=Deaths_in_February_2004|Deaths_in_October_2004|Deaths_in_January_2004|Deaths_in_April_2004|Deaths_in_November_2004|Deaths_in_March_2004|Deaths_in_August_2004|Deaths_in_May_2004|Deaths_in_December_2004|Deaths_in_June_2004|Deaths_in_September_2004|Deaths_in_July_2004&prop=revisions&rvprop=content&format=json'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_url1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_raw1 = requests.get(batch_url1).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get page names for each month\n",
    "page_names = [date_elem.values()[0] for date_elem in batch_raw1['query']['normalized']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All years:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_url_all = base_url_prefix + batch_year_str + base_url_suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 68.2 ms, sys: 15.2 ms, total: 83.5 ms\n",
      "Wall time: 1.28 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "batch_raw_all = requests.get(batch_url_all).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "page_names_all = [date_elem.values()[0] for date_elem in batch_raw_all['query']['normalized']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "month_year_lists = {}\n",
    "\n",
    "num_months = len(page_names_all)\n",
    "for i in xrange(0,num_months):\n",
    "    month_str = page_names_all[i]\n",
    "    my_key = '2111_22' # need to change to match month_str\n",
    "    raw_page = batch_raw_all['query']['pages'].values()[i]['revisions'][0]['*']\n",
    "    \n",
    "    month_year_lists[month_str] = [\n",
    "        add_description_and_death(\n",
    "            parse_month_year_name_age(my_key, \n",
    "                                      re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', \n",
    "                                             entry, flags=re.MULTILINE)\n",
    "                                     )\n",
    "        )\n",
    "        for entry in raw_page.encode('utf-8').rstrip().split('*')\n",
    "        if re.match(name_age_re, entry.replace('\\n', ''))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(month_year_lists.values()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_raw_all['query']['pages'].values()[0]['revisions'][0]['*']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Pooling Multi-Processing Scrape\n",
    "\n",
    "For additional speed. However, might be very taxing on Wikipedia servers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
